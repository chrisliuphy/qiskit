{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data shape: (20000, 1, 101, 2)\n",
      "labels shape: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 加载数据\n",
    "with h5py.File('C:/Users/liusi/Desktop/research/transformer/dataset.h5', 'r') as f:\n",
    "    input_data = f['input_data'][:]  # 读取输入数据\n",
    "\n",
    "    # 检查labels是否为标量数据集\n",
    "    if f['labels'].shape == ():  # 如果是标量\n",
    "        labels = f['labels'][()]  # 读取单个标量值\n",
    "    else:\n",
    "        labels = f['labels'][:]  # 读取数组\n",
    "\n",
    "print(\"input_data shape:\", input_data.shape)\n",
    "print(\"labels shape:\", labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建自定义数据集\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = torch.LongTensor(labels)  # 转换为长整型\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# 3. 划分训练和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(input_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建Dataset对象\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "\n",
    "# 4. 创建DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels):\n",
    "        super(CNN, self).__init__()\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=40, kernel_size=(2, 2), stride=1, padding=0), # 输出尺寸：(N, 40, H-1, W-1)\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 1), stride=(1, 1)) # 这个池化层可能不会改变尺寸，取决于实际需求\n",
    "        )\n",
    "        self.cnn2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=40, out_channels=1, kernel_size=(1, 1), stride=1, padding=0), # 输出尺寸：(N, 1, H-1, W-1)\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(1, 1), stride=(1, 1))  # 这个池化层可能不会改变尺寸，取决于实际需求\n",
    "        )\n",
    "        # 在此全连接层之前，需要知道上一层的输出尺寸\n",
    "        # 假设上一层的输出尺寸是 (1, H', W')\n",
    "        # 全连接层的输入尺寸将是 1 * H' * W'\n",
    "        self.fc1 = nn.Linear(1 * 100 * 1, 2)  # 全连接层有2个神经元\n",
    "        self.fc2 = nn.Linear(2, num_classes)  # 输出层\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.cnn2(out)\n",
    "        out = out.view(out.size(0), -1)  # 展平\n",
    "        out = F.relu(self.fc1(out))  # 应用ReLU激活函数\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 40, 100, 1]             200\n",
      "       BatchNorm2d-2           [-1, 40, 100, 1]              80\n",
      "              ReLU-3           [-1, 40, 100, 1]               0\n",
      "         MaxPool2d-4           [-1, 40, 100, 1]               0\n",
      "            Conv2d-5            [-1, 1, 100, 1]              41\n",
      "       BatchNorm2d-6            [-1, 1, 100, 1]               2\n",
      "              ReLU-7            [-1, 1, 100, 1]               0\n",
      "         MaxPool2d-8            [-1, 1, 100, 1]               0\n",
      "            Linear-9                    [-1, 2]             202\n",
      "           Linear-10                    [-1, 5]              15\n",
      "================================================================\n",
      "Total params: 540\n",
      "Trainable params: 540\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 模型实例化\n",
    "num_classes = 5  # 类别数\n",
    "in_channel = 1   # 输入通道数\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(num_classes=num_classes, in_channels=in_channel).to(device)\n",
    "summary(model, input_size=(in_channel, 101, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "criterior = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/200, step 250/250, loss = 1.0340\n",
      "2/200, step 250/250, loss = 0.8521\n",
      "3/200, step 250/250, loss = 0.9797\n",
      "4/200, step 250/250, loss = 0.9649\n",
      "5/200, step 250/250, loss = 0.8700\n",
      "6/200, step 250/250, loss = 0.9954\n",
      "7/200, step 250/250, loss = 0.9822\n",
      "8/200, step 250/250, loss = 0.9689\n",
      "9/200, step 250/250, loss = 0.9217\n",
      "10/200, step 250/250, loss = 1.0576\n",
      "11/200, step 250/250, loss = 0.9150\n",
      "12/200, step 250/250, loss = 0.9883\n",
      "13/200, step 250/250, loss = 0.9912\n",
      "14/200, step 250/250, loss = 0.8992\n",
      "15/200, step 250/250, loss = 0.9374\n",
      "16/200, step 250/250, loss = 1.0246\n",
      "17/200, step 250/250, loss = 1.0108\n",
      "18/200, step 250/250, loss = 1.1015\n",
      "19/200, step 250/250, loss = 0.9877\n",
      "20/200, step 250/250, loss = 1.1011\n",
      "21/200, step 250/250, loss = 0.8611\n",
      "22/200, step 250/250, loss = 0.9076\n",
      "23/200, step 250/250, loss = 0.9029\n",
      "24/200, step 250/250, loss = 0.9554\n",
      "25/200, step 250/250, loss = 0.9402\n",
      "26/200, step 250/250, loss = 0.9564\n",
      "27/200, step 250/250, loss = 0.9494\n",
      "28/200, step 250/250, loss = 0.9585\n",
      "29/200, step 250/250, loss = 0.9786\n",
      "30/200, step 250/250, loss = 0.9320\n",
      "31/200, step 250/250, loss = 0.9004\n",
      "32/200, step 250/250, loss = 0.9534\n",
      "33/200, step 250/250, loss = 1.0595\n",
      "34/200, step 250/250, loss = 0.9847\n",
      "35/200, step 250/250, loss = 0.9320\n",
      "36/200, step 250/250, loss = 1.0374\n",
      "37/200, step 250/250, loss = 0.9632\n",
      "38/200, step 250/250, loss = 0.8661\n",
      "39/200, step 250/250, loss = 0.9346\n",
      "40/200, step 250/250, loss = 0.9427\n",
      "41/200, step 250/250, loss = 1.0041\n",
      "42/200, step 250/250, loss = 0.9700\n",
      "43/200, step 250/250, loss = 1.1342\n",
      "44/200, step 250/250, loss = 0.8718\n",
      "45/200, step 250/250, loss = 1.1520\n",
      "46/200, step 250/250, loss = 1.0130\n",
      "47/200, step 250/250, loss = 0.9441\n",
      "48/200, step 250/250, loss = 1.0037\n",
      "49/200, step 250/250, loss = 0.9177\n",
      "50/200, step 250/250, loss = 1.0068\n",
      "51/200, step 250/250, loss = 0.9299\n",
      "52/200, step 250/250, loss = 1.0034\n",
      "53/200, step 250/250, loss = 0.9194\n",
      "54/200, step 250/250, loss = 0.8979\n",
      "55/200, step 250/250, loss = 0.9263\n",
      "56/200, step 250/250, loss = 0.8426\n",
      "57/200, step 250/250, loss = 0.8930\n",
      "58/200, step 250/250, loss = 0.9604\n",
      "59/200, step 250/250, loss = 0.9000\n",
      "60/200, step 250/250, loss = 1.0571\n",
      "61/200, step 250/250, loss = 1.0294\n",
      "62/200, step 250/250, loss = 0.8579\n",
      "63/200, step 250/250, loss = 0.8742\n",
      "64/200, step 250/250, loss = 0.9493\n",
      "65/200, step 250/250, loss = 0.8734\n",
      "66/200, step 250/250, loss = 0.9056\n",
      "67/200, step 250/250, loss = 1.0850\n",
      "68/200, step 250/250, loss = 0.9139\n",
      "69/200, step 250/250, loss = 1.0321\n",
      "70/200, step 250/250, loss = 0.9627\n",
      "71/200, step 250/250, loss = 0.8987\n",
      "72/200, step 250/250, loss = 1.0367\n",
      "73/200, step 250/250, loss = 0.9491\n",
      "74/200, step 250/250, loss = 0.9005\n",
      "75/200, step 250/250, loss = 0.9446\n",
      "76/200, step 250/250, loss = 1.0333\n",
      "77/200, step 250/250, loss = 0.8627\n",
      "78/200, step 250/250, loss = 1.0604\n",
      "79/200, step 250/250, loss = 1.0506\n",
      "80/200, step 250/250, loss = 0.9761\n",
      "81/200, step 250/250, loss = 0.9305\n",
      "82/200, step 250/250, loss = 1.0805\n",
      "83/200, step 250/250, loss = 0.8830\n",
      "84/200, step 250/250, loss = 1.1844\n",
      "85/200, step 250/250, loss = 0.9926\n",
      "86/200, step 250/250, loss = 1.0709\n",
      "87/200, step 250/250, loss = 0.9756\n",
      "88/200, step 250/250, loss = 0.9756\n",
      "89/200, step 250/250, loss = 1.0212\n",
      "90/200, step 250/250, loss = 0.9861\n",
      "91/200, step 250/250, loss = 0.9688\n",
      "92/200, step 250/250, loss = 0.9731\n",
      "93/200, step 250/250, loss = 0.9238\n",
      "94/200, step 250/250, loss = 0.8936\n",
      "95/200, step 250/250, loss = 1.0056\n",
      "96/200, step 250/250, loss = 0.9704\n",
      "97/200, step 250/250, loss = 1.0069\n",
      "98/200, step 250/250, loss = 0.9027\n",
      "99/200, step 250/250, loss = 0.9425\n",
      "100/200, step 250/250, loss = 0.9889\n",
      "101/200, step 250/250, loss = 0.9434\n",
      "102/200, step 250/250, loss = 0.9640\n",
      "103/200, step 250/250, loss = 0.9037\n",
      "104/200, step 250/250, loss = 0.9048\n",
      "105/200, step 250/250, loss = 1.0488\n",
      "106/200, step 250/250, loss = 0.8786\n",
      "107/200, step 250/250, loss = 0.9578\n",
      "108/200, step 250/250, loss = 0.8741\n",
      "109/200, step 250/250, loss = 1.0381\n",
      "110/200, step 250/250, loss = 1.0045\n",
      "111/200, step 250/250, loss = 1.0240\n",
      "112/200, step 250/250, loss = 0.8985\n",
      "113/200, step 250/250, loss = 0.9306\n",
      "114/200, step 250/250, loss = 0.9993\n",
      "115/200, step 250/250, loss = 0.9051\n",
      "116/200, step 250/250, loss = 0.9628\n",
      "117/200, step 250/250, loss = 0.9933\n",
      "118/200, step 250/250, loss = 0.9236\n",
      "119/200, step 250/250, loss = 1.0689\n",
      "120/200, step 250/250, loss = 1.1120\n",
      "121/200, step 250/250, loss = 0.9248\n",
      "122/200, step 250/250, loss = 0.9647\n",
      "123/200, step 250/250, loss = 0.8996\n",
      "124/200, step 250/250, loss = 0.9349\n",
      "125/200, step 250/250, loss = 1.0260\n",
      "126/200, step 250/250, loss = 0.9329\n",
      "127/200, step 250/250, loss = 0.9551\n",
      "128/200, step 250/250, loss = 1.0198\n",
      "129/200, step 250/250, loss = 0.9894\n",
      "130/200, step 250/250, loss = 1.0063\n",
      "131/200, step 250/250, loss = 0.8826\n",
      "132/200, step 250/250, loss = 1.0592\n",
      "133/200, step 250/250, loss = 0.9580\n",
      "134/200, step 250/250, loss = 1.0037\n",
      "135/200, step 250/250, loss = 0.9719\n",
      "136/200, step 250/250, loss = 1.0667\n",
      "137/200, step 250/250, loss = 1.0018\n",
      "138/200, step 250/250, loss = 0.9641\n",
      "139/200, step 250/250, loss = 0.9819\n",
      "140/200, step 250/250, loss = 0.9466\n",
      "141/200, step 250/250, loss = 1.0111\n",
      "142/200, step 250/250, loss = 0.8961\n",
      "143/200, step 250/250, loss = 0.9725\n",
      "144/200, step 250/250, loss = 0.8364\n",
      "145/200, step 250/250, loss = 0.9422\n",
      "146/200, step 250/250, loss = 0.9555\n",
      "147/200, step 250/250, loss = 0.9065\n",
      "148/200, step 250/250, loss = 1.0230\n",
      "149/200, step 250/250, loss = 1.1139\n",
      "150/200, step 250/250, loss = 0.9729\n",
      "151/200, step 250/250, loss = 0.9904\n",
      "152/200, step 250/250, loss = 0.8632\n",
      "153/200, step 250/250, loss = 0.9451\n",
      "154/200, step 250/250, loss = 0.9176\n",
      "155/200, step 250/250, loss = 1.0465\n",
      "156/200, step 250/250, loss = 0.8502\n",
      "157/200, step 250/250, loss = 1.0024\n",
      "158/200, step 250/250, loss = 0.9564\n",
      "159/200, step 250/250, loss = 1.0105\n",
      "160/200, step 250/250, loss = 0.8411\n",
      "161/200, step 250/250, loss = 0.9510\n",
      "162/200, step 250/250, loss = 0.9535\n",
      "163/200, step 250/250, loss = 0.9103\n",
      "164/200, step 250/250, loss = 0.9349\n",
      "165/200, step 250/250, loss = 0.9417\n",
      "166/200, step 250/250, loss = 0.9753\n",
      "167/200, step 250/250, loss = 0.9223\n",
      "168/200, step 250/250, loss = 1.0528\n",
      "169/200, step 250/250, loss = 0.8946\n",
      "170/200, step 250/250, loss = 0.9137\n",
      "171/200, step 250/250, loss = 1.0649\n",
      "172/200, step 250/250, loss = 0.9538\n",
      "173/200, step 250/250, loss = 0.9571\n",
      "174/200, step 250/250, loss = 0.9302\n",
      "175/200, step 250/250, loss = 1.0119\n",
      "176/200, step 250/250, loss = 0.8903\n",
      "177/200, step 250/250, loss = 0.9708\n",
      "178/200, step 250/250, loss = 1.1430\n",
      "179/200, step 250/250, loss = 1.0677\n",
      "180/200, step 250/250, loss = 1.0994\n",
      "181/200, step 250/250, loss = 0.9164\n",
      "182/200, step 250/250, loss = 0.8905\n",
      "183/200, step 250/250, loss = 0.9795\n",
      "184/200, step 250/250, loss = 0.9312\n",
      "185/200, step 250/250, loss = 0.9701\n",
      "186/200, step 250/250, loss = 0.8785\n",
      "187/200, step 250/250, loss = 0.9703\n",
      "188/200, step 250/250, loss = 0.8377\n",
      "189/200, step 250/250, loss = 0.9673\n",
      "190/200, step 250/250, loss = 0.8880\n",
      "191/200, step 250/250, loss = 1.1512\n",
      "192/200, step 250/250, loss = 0.8022\n",
      "193/200, step 250/250, loss = 0.8842\n",
      "194/200, step 250/250, loss = 0.8964\n",
      "195/200, step 250/250, loss = 0.9556\n",
      "196/200, step 250/250, loss = 1.0333\n",
      "197/200, step 250/250, loss = 0.9044\n",
      "198/200, step 250/250, loss = 0.9684\n",
      "199/200, step 250/250, loss = 0.9906\n",
      "200/200, step 250/250, loss = 0.9606\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(images)\n",
    "        loss = criterior(outputs, labels)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad() # 梯度清零\n",
    "        loss.backward() # 反向传播\n",
    "\n",
    "        # update\n",
    "        optimizer.step() # 更新参数\n",
    "\n",
    "        if (batch_idx+1) % 250 == 0:\n",
    "            print(f\"{epoch+1}/{num_epochs}, step {batch_idx+1}/{len(train_loader)}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载测试数据\n",
    "with h5py.File('C:/Users/liusi/Desktop/research/transformer/dataset_test.h5', 'r') as f:\n",
    "    test_input_data = f['input_data'][:]  # 假设测试集的特征数据键是 'input_data'\n",
    "    test_labels = f['labels'][:]  # 假设测试集的标签数据键是 'labels'\n",
    "\n",
    "# 2. 创建测试集 Dataset\n",
    "test_dataset = CustomDataset(test_input_data, test_labels)\n",
    "\n",
    "# 3. 创建测试集 DataLoader\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)  # 通常测试集不需要打乱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10034/20000, accuracy = 0.5017\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for images, labels in test_loader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    outputs = model(images)\n",
    "    predicted = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels).sum().item()\n",
    "    \n",
    "print(f'{correct}/{total}, accuracy = {correct/total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
